{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JLDynamics/CV/blob/main/z_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ExlUP6CrUM"
      },
      "source": [
        "# Z-Image-Turbo Image Generation in Colab\n",
        "\n",
        "This notebook contains the complete working code to generate images using Z-Image-Turbo in Google Colab.\n",
        "\n",
        "‚úÖ **All setup complete!** Just run the cells below to start generating images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RCdhVlKZCrUN"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages and set up Google Drive\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "!pip install torch accelerate transformers pillow\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# Mount Google Drive to access the model\n",
        "print(\"üíæ Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V8Xeb25CrUN"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import libraries and check environment\n",
        "import torch\n",
        "from diffusers import ZImagePipeline\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "print(\"üîç Checking environment...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected, using CPU (will be very slow)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyuNY9yQCrUO"
      },
      "outputs": [],
      "source": [
        "# Step 3: Set up model path and copy from Google Drive\n",
        "print(\"üìÅ Setting up model path...\")\n",
        "\n",
        "# Define where the model is stored in your Google Drive\n",
        "# Replace this path with where you uploaded the Z-Image-Turbo folder\n",
        "drive_model_path = \"/content/drive/MyDrive/Z-Image-Turbo\"\n",
        "colab_model_path = \"/content/Z-Image-Turbo\"\n",
        "\n",
        "# Check if model exists in Google Drive\n",
        "if os.path.exists(drive_model_path):\n",
        "    print(f\"‚úÖ Model found in Google Drive at: {drive_model_path}\")\n",
        "\n",
        "    # Copy model from Google Drive to Colab workspace\n",
        "    print(\"üìÇ Copying model to Colab workspace...\")\n",
        "    !cp -r \"{drive_model_path}\" \"{colab_model_path}\"\n",
        "    print(\"‚úÖ Model copied successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model not found in Google Drive at: {drive_model_path}\")\n",
        "    print(\"Please upload the Z-Image-Turbo folder to your Google Drive first\")\n",
        "    print(\"Then update the drive_model_path variable above to match your upload location\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKR2Vj6_CrUO"
      },
      "outputs": [],
      "source": [
        "# Step 4: Load the Z-Image-Turbo model\n",
        "print(\"üöÄ Loading Z-Image-Turbo model...\")\n",
        "\n",
        "# Clear CUDA cache and collect garbage to free up memory\n",
        "import gc\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"üßπ Cleared CUDA cache and collected garbage.\")\n",
        "\n",
        "try:\n",
        "    # Load with appropriate dtype based on available hardware\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    pipe = ZImagePipeline.from_pretrained(\n",
        "        colab_model_path,  # Use the copied model path\n",
        "        torch_dtype=dtype,\n",
        "        low_cpu_mem_usage=True, # Changed to True for memory optimization\n",
        "    )\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        pipe.to(\"cuda\")\n",
        "\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "    # Optional: Enable Flash Attention for better performance\n",
        "    try:\n",
        "        pipe.transformer.set_attention_backend(\"flash\")\n",
        "        print(\"‚ö° Flash Attention enabled\")\n",
        "    except:\n",
        "        print(\"üîß Flash Attention not available\")\n",
        "\n",
        "    # Optional: Compile model for faster inference (disabled for troubleshooting shape mismatch)\n",
        "    # try:\n",
        "    #     pipe.transformer.compile()\n",
        "    #     print(\"üî® Model compiled for faster execution\")\n",
        "    # except:\n",
        "    #     print(\"üîß Model compilation not available\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load model: {e}\")\n",
        "    print(f\"Please ensure the model is properly copied to: {colab_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33a5e3ea"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Define resolution map\n",
        "resolution_map = {\n",
        "    'Low': (512, 512),\n",
        "    'Medium': (768, 768),\n",
        "    'High': (1024, 1024),\n",
        "    '1080p': (1920, 1080)\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Libraries imported and resolution map defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a0dfd09"
      },
      "source": [
        "print(\"‚öôÔ∏è Creating interactive interface...\")\n",
        "\n",
        "# Create widgets\n",
        "prompt_widget = widgets.Textarea(\n",
        "    value=\"A majestic lion roaring in the savanna, golden hour, highly detailed, photorealistic\",\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='auto', height='100px')\n",
        ")\n",
        "\n",
        "resolution_selector = widgets.Dropdown(\n",
        "    options=list(resolution_map.keys()),\n",
        "    value='High',\n",
        "    description='Resolution:'\n",
        ")\n",
        "\n",
        "num_inference_steps_widget = widgets.IntSlider(\n",
        "    value=9,\n",
        "    min=1,\n",
        "    max=20,\n",
        "    step=1,\n",
        "    description='Inference Steps:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "random_seed_widget = widgets.IntText(\n",
        "    value=42,\n",
        "    description='Random Seed:'\n",
        ")\n",
        "\n",
        "generate_button = widgets.Button(\n",
        "    description='Generate Image',\n",
        "    button_style='success',\n",
        "    icon='magic'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_generate_button_clicked(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        current_prompt = prompt_widget.value\n",
        "        selected_resolution_key = resolution_selector.value\n",
        "        current_num_inference_steps = num_inference_steps_widget.value\n",
        "        current_random_seed = random_seed_widget.value\n",
        "\n",
        "        width, height = resolution_map[selected_resolution_key]\n",
        "\n",
        "        print(\"Generating image...\")\n",
        "        print(f\"Prompt: {current_prompt[:100]}...\")\n",
        "        print(f\"Resolution: {width}x{height}\")\n",
        "        print(f\"Inference Steps: {current_num_inference_steps}\")\n",
        "        print(f\"Random Seed: {current_random_seed}\")\n",
        "\n",
        "        if selected_resolution_key == '1080p':\n",
        "            print(\"‚ö†Ô∏è Warning: High resolution (1080p) might be slow and require significant VRAM.\")\n",
        "\n",
        "        try:\n",
        "            generator = torch.Generator(device).manual_seed(current_random_seed)\n",
        "            image = pipe(\n",
        "                prompt=current_prompt,\n",
        "                height=height,\n",
        "                width=width,\n",
        "                num_inference_steps=current_num_inference_steps,\n",
        "                guidance_scale=0.0,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "\n",
        "            display(image)\n",
        "            output_filename = f\"generated_image_{width}x{height}_seed{current_random_seed}.png\"\n",
        "            image.save(output_filename)\n",
        "            print(f\"‚úÖ Image saved as: {output_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Image generation failed: {e}\")\n",
        "\n",
        "generate_button.on_click(on_generate_button_clicked)\n",
        "\n",
        "display(\n",
        "    widgets.VBox([\n",
        "        prompt_widget,\n",
        "        resolution_selector,\n",
        "        num_inference_steps_widget,\n",
        "        random_seed_widget,\n",
        "        generate_button,\n",
        "        output_area\n",
        "    ])\n",
        ")\n",
        "print(\"‚úÖ Interactive interface created.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}